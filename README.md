# üì∏ Gallery Agent: The AI That Remembers
### *Stop Scrolling. Start Finding.*

[![Watch the Demo](https://img.youtube.com/vi/efWvyoxhFoo/0.jpg)](https://youtu.be/efWvyoxhFoo)
> **Click above to watch the 3-minute project walkthrough and demo.**

---

## 1. The Pitch (Problem, Solution, Value)

### üî¥ The Problem: The "Digital Landfill"
We have become hoarders of our own history. The average user has thousands of photos buried in their digital galleries. Finding a specific memory‚Äîlike *"my first anniversary pic"* or *"me in the blue dress fishing at the lake"*‚Äîis a manual, frustrating process.
* **Keyword search fails:** It doesn't know "blue dress" if you didn't manually tag it.
* **Face clustering is limited:** It knows *who* is there, but not *what* they are doing.
* **The Result:** We spend more time scrolling through screenshots and duplicates than actually reminiscing.

### üü¢ The Solution: Semantic Auto-Tagging
**Gallery Agent** is an intelligent, multi-agent system that ingests photos and understands them like a human does. By combining **Visual Identity (Face Recognition)** with **Semantic Understanding of Summary (Gemini AI)**, it turns your messy photo folder into a structured, searchable knowledge base.

**How it works:**
So, how does our system actually "see" your photos? It‚Äôs not magic. It‚Äôs teamwork between our specialized models working in the background.

**Storing the images**

1.  **Ingestion & Semantic Understanding:** First, we generate a UUID and store the image on our local disk. For semantic understanding, we leverage **Google's Gemini multimodal models**. It analyzes the scene's context‚Äîactions, objects, and mood‚Äîand generates a detailed text summary.
2.  **Face Identity:** After this, we have our **"Face_id tool."** We use a powerful model called **Facenet512**. It maps the unique geometry of a face and turns it into a precise mathematical code‚Äîa unique face ID for every person it sees.
3.  **Storage & Indexing:** By combining these identity vectors with semantic descriptions in our database, we turn your messy photo folder into a structured, searchable knowledge base. Once we have all the information, the photo is uploaded to **Cloudinary**, the local copy is deleted, and all data is sent to the **Qdrant** database for searching.

**Searching the Images**

The system offers three flexible ways to find your memories:

1.  **Text-Only Search:** Simply describe what you're looking for (e.g., *"A sunset at the beach"*). The system searches the semantic summaries generated by Gemini.
2.  **Face-Only Search:** Upload a photo of a person. The system extracts their unique Face ID using `Facenet512` and retrieves all photos containing that person.
3.  **Hybrid Search (Face + Text):** The most powerful option. Upload a reference photo *and* add a text query (e.g., *"This person eating pizza"*).

**How Hybrid Search Works:**
1.  **Identity Extraction:** The `FaceUUIDTool` analyzes the reference photo to extract the face vector.
2.  **Database Lookup:** It checks if this identity(the face_id) exists in our Qdrant database. If the person isn't in your gallery, it returns "No images found."
3.  **Semantic Intersection:** If the person is found, the system retrieves all their photos and then filters them using the text query (e.g., "eating pizza") against the stored semantic summaries. If there is no query, then all the images with the given face are returned.
4.  **Result:** The system returns the specific images that match both the *who* (Face ID) and the *what* (Semantic Description).

### üíé The Value Proposition
* **Complex Queries:** You can ask, *"Find the photo where Sandeep is eating pizza."* The system intersects the identity of "Sandeep" (from a reference photo) with the semantic concept of "eating pizza."
* **Privacy-First:** Face recognition logic runs locally using DeepFace; only the scene summarization is sent to the LLM.
* **Zero-Shot Organization:** No manual tagging required. Drop a folder of images, and the agents do the rest.

---

## 2. Core Concept & Value
Innovation & Agentic Relevance

### Why Agents? (Beyond simple scripts)
This project utilizes a **Multi-Agent Orchestration** architecture. A simple script cannot handle the ambiguity of visual data or the decision-making required for identity resolution.

**1. Agentic Reasoning & Decision Making**
The "Store Agent" doesn't just save data; it makes active decisions:
* **Threshold Logic:** It calculates vector distances (Cosine Similarity) to decide: *Is this a new person, or someone we know?* If the distance is < 0.5, it retrieves the existing UUID. If > 0.5, it creates a new identity.
* **Validation:** It validates image integrity before passing data to the storage pipeline.

**2. Tool Use & Specialization**
We utilized the "Specialist" pattern because a generalist LLM cannot measure face geometry accurately, and a computer vision model cannot understand semantic context.
* **Tool A (FaceUUIDTool):** A specialized Python tool using `Facenet512` to handle biometric vectors.
* **Tool B (GeminiSummaryTool):** A multimodal tool using `Gemini 2.5 Flash` for context understanding.
* **Root Agent:** Orchestrates the complex handoff between these tools and the database.

**3. The Innovation: Hybrid Vector Intersection & Agentic Verification**
Most gallery apps do *either* face search *or* keyword search. Our innovation is the intersection and intelligent verification:
> `Final Result = Vector_Search(Description) ‚à© Metadata_Filter(Face_UUID)`

**The Two-Step Search Process:**
1.  **Broad Retrieval:** We first perform a vector search on the semantic summaries with a **similarity threshold of 0.6**. This casts a wide net to catch all potentially relevant images.
2.  **Agentic Re-Ranking:** We don't just trust the vectors. We pass the user's query and the summaries of the retrieved images to **Gemini 2.5**. The model evaluates if the image *truly* answers the specific nuance of the query, filtering out false positives to give the user exactly what they asked for.

---

## 3. The Writeup
*(Category 3: 15 Points - Architecture & Journey)*

### üèóÔ∏è Architecture & Workflow

![Workflow Diagram](path/to/your/workflow_diagram.jpg)
*(Replace the path above with your actual diagram file)*

The system is built on a **Root Agent** dispatch model that manages two primary flows:

**Flow A: The Ingestion Pipeline (Save Image Agent)**
1.  **Input:** User uploads an image.
2.  **Parallel Processing:**
    * **Identity:** The `FaceUUIDTool` scans the image. If the face vector matches a known user, it returns the UUID. If not, it registers a new UUID.
    * **Context:** The `Gemini Tool` analyzes the scene: *"A man sitting at a cafe table, smiling, holding a slice of pepperoni pizza."*
    * **Storage:** The image is uploaded to **Cloudinary** (returning a URL).
3.  **Vectorization:** The `Save in DB Agent` packages the UUIDs, Summary, and URL into a payload and upserts it to **Qdrant**.

**Flow B: The Search Pipeline (Search Image Agent)**
1.  **User Query:** *"Show me Sandeep eating pizza"* + [Reference Photo of Sandeep].
2.  **Target Extraction:** The agent extracts the UUID from the reference photo using the Face Tool.
3.  **Hybrid Retrieval:** It queries Qdrant for vectors matching "eating pizza" *while filtering* for the specific UUID.
4.  **Result:** The exact image is returned.

### üó∫Ô∏è Project Journey & Challenges

**The "Human-in-the-Loop" Pivot**
Initially, we designed the agent to pop up a window asking, *"Who is this?"* whenever it saw a new face. We realized this was unscalable for a gallery of 1,000+ photos.
* **The Pivot:** We shifted to an **Auto-Tagging UUID system**. The agent now assigns anonymous IDs (`uuid-1234`) automatically. We can search by providing a reference photo later, removing the need for manual data entry during ingestion.

**The "Blind Agent" Problem**
Many agent frameworks are text-first and cannot "hold" an image file in memory.
* **The Solution:** We implemented a **File Path Handoff pattern**. The Agent handles the *path string* (e.g., `./images/photo.jpg`), while the custom Python tools handle the actual byte-level processing (OpenCV/DeepFace/Gemini API).

### üõ†Ô∏è Tech Stack
* **Agent Brain:** Google Gemini 2.5 Flash
* **Vision/Biometrics:** DeepFace (Facenet512 + Retina Backend)
* **Memory (Vector DB):** Qdrant
* **Storage:** Cloudinary
* **Backend:** Python 3.10

---

## 4. Setup & Usage

### Prerequisites
* Python 3.10+
* Google Gemini API Key
* Cloudinary Account
* Qdrant Instance (Docker or Cloud)

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/yourusername/gallery-agent.git
    cd gallery-agent
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configuration:**
    Create a `.env` file in the root directory:
    ```env
    GOOGLE_API_KEY=your_gemini_key
    CLOUDINARY_CLOUD_NAME=your_cloud_name
    CLOUDINARY_API_KEY=your_key
    CLOUDINARY_API_SECRET=your_secret
    QDRANT_URL=your_qdrant_url
    QDRANT_API_KEY=your_qdrant_key
    ```
