# üì∏ Gallery Agent: The AI That Remembers

### *Stop Scrolling. Start Finding.*

[![Watch the Demo](https://img.youtube.com/vi/efWvyoxhFoo/0.jpg)](https://youtu.be/efWvyoxhFoo)

> **Click above to watch the 3-minute project walkthrough and demo.**

---

## 1. The Pitch (Problem, Solution, Value)

### üî¥ The Problem: The "Digital Landfill"

We have become hoarders of our own history. The average user has thousands of photos buried in their digital galleries. Finding a specific memory‚Äîlike *"my first anniversary pic"* or *"me in the blue dress fishing at the lake"*‚Äîis a manual, frustrating process.

* **Keyword search fails:** It doesn't know "blue dress" if you didn't manually tag it.
* **Face clustering is limited:** It knows *who* is there, but not *what* they are doing.
* **The Result:** We spend more time scrolling through screenshots and duplicates than actually reminiscing.

### üü¢ The Solution: Semantic Auto-Tagging

**Gallery Agent** is an intelligent, multi-agent system that ingests photos and understands them like a human does. By combining **Visual Identity (Face Recognition)** with **Semantic Understanding of Summary (Gemini AI)**, it turns your messy photo folder into a structured, searchable knowledge base.

**How it works:**

So, how does our system actually "see" your photos? It's not magic. It's teamwork between our specialized models working in the background.

#### **Storing the Images**

1. **Ingestion & Semantic Understanding:** First, we generate a UUID and store the image on our local disk. For semantic understanding, we leverage **Google's Gemini multimodal models**. It analyzes the scene's context‚Äîactions, objects, and mood‚Äîand generates a detailed text summary.

2. **Face Identity:** After this, we have our **"Face_id tool."** We use a powerful model called **Facenet512**. It maps the unique geometry of a face and turns it into a precise mathematical code‚Äîa unique face ID for every person it sees.

3. **Storage & Indexing:** By combining these identity vectors with semantic descriptions in our database, we turn your messy photo folder into a structured, searchable knowledge base. Once we have all the information, the photo is uploaded to **Cloudinary**, the local copy is deleted, and all data is sent to the **Qdrant** database for searching.

#### **Searching the Images**

The system offers three flexible ways to find your memories:

1. **Text-Only Search:** Simply describe what you're looking for (e.g., *"A sunset at the beach"*). The system searches the semantic summaries generated by Gemini.

2. **Face-Only Search:** Upload a photo of a person. The system extracts their unique Face ID using `Facenet512` and retrieves all photos containing that person.

3. **Hybrid Search (Face + Text):** The most powerful option. Upload a reference photo *and* add a text query (e.g., *"This person eating pizza"*).

**How Hybrid Search Works:**

1. **Identity Extraction:** The `FaceUUIDTool` analyzes the reference photo to extract the face vector.
2. **Database Lookup:** It checks if this identity (the face_id) exists in our Qdrant database. If the person isn't in your gallery, it returns "No images found."
3. **Semantic Intersection:** If the person is found, the system retrieves all their photos and then filters them using the text query (e.g., "eating pizza") against the stored semantic summaries. If there is no query, then all the images with the given face are returned.
4. **Result:** The system returns the specific images that match both the *who* (Face ID) and the *what* (Semantic Description).

### üíé The Value Proposition

* **Complex Queries:** You can ask, *"Find the photo where Sandeep is eating pizza."* The system intersects the identity of "Sandeep" (from a reference photo) with the semantic concept of "eating pizza."
* **Privacy-First:** Face recognition logic runs locally using DeepFace; only the scene summarization is sent to the LLM.
* **Zero-Shot Organization:** No manual tagging required. Drop a folder of images, and the agents do the rest.

---

## 2. Core Concept & Value Innovation & Agentic Relevance

### Why Agents? (Beyond simple scripts)

This project utilizes a **Multi-Agent Orchestration** architecture. A simple script cannot handle the ambiguity of visual data or the decision-making required for identity resolution.

**1. Agentic Reasoning & Decision Making**

The "Store Agent" doesn't just save data; it makes active decisions:

* **Threshold Logic:** It calculates vector distances (Cosine Similarity) to decide: *Is this a new person, or someone we know?* If the distance is < 0.5, it retrieves the existing UUID. If > 0.5, it creates a new identity.
* **Validation:** It validates image integrity before passing data to the storage pipeline.

**2. Tool Use & Specialization**

We utilized the "Specialist" pattern because a generalist LLM cannot measure face geometry accurately, and a computer vision model cannot understand semantic context.

* **Tool A (FaceUUIDTool):** A specialized Python tool using `Facenet512` to handle biometric vectors.
* **Tool B (GeminiSummaryTool):** A multimodal tool using `Gemini 2.5 Flash` for context understanding.
* **Root Agent:** Orchestrates the complex handoff between these tools and the database.

**3. The Innovation: Hybrid Vector Intersection & Agentic Verification**

Most gallery apps do *either* face search *or* keyword search. Our innovation is the intersection and intelligent verification:

> `Final Result = Vector_Search(Description) ‚à© Metadata_Filter(Face_UUID)`

**The Two-Step Search Process:**

1. **Broad Retrieval:** We first perform a vector search on the semantic summaries with a **similarity threshold of 0.6**. This casts a wide net to catch all potentially relevant images.

2. **Agentic Re-Ranking:** We don't just trust the vectors. We pass the user's query and the summaries of the retrieved images to **Gemini 2.5**. The model evaluates if the image *truly* answers the specific nuance of the query, filtering out false positives to give the user exactly what they asked for.

---

## 3. The Writeup
*(Category 3: 15 Points - Architecture & Journey)*

### üèóÔ∏è Architecture & Workflow

![Workflow Diagram](https://res.cloudinary.com/dqcgbfxki/image/upload/v1764365450/Gemini_Generated_Image_1zjiq81zjiq81zji_q4smnx.png)

The system is built on a **Root Agent** dispatch model that manages two primary flows:

**Flow A: The Ingestion Pipeline (Save Image Agent)**

1. **Input:** User uploads an image.
2. **Parallel Processing:**
   * **Identity:** The `FaceUUIDTool` scans the image. If the face vector matches a known user, it returns the UUID. If not, it registers a new UUID.
   * **Context:** The `Gemini Tool` analyzes the scene: *"A man sitting at a cafe table, smiling, holding a slice of pepperoni pizza."*
   * **Storage:** The image is uploaded to **Cloudinary** (returning a URL).
3. **Vectorization:** The `Save in DB Agent` packages the UUIDs, Summary, and URL into a payload and upserts it to **Qdrant**.

**Flow B: The Search Pipeline (Search Image Agent)**

1. **User Query:** *"Show me Sandeep eating pizza"* + [Reference Photo of Sandeep].
2. **Target Extraction:** The agent extracts the UUID from the reference photo using the Face Tool.
3. **Hybrid Retrieval:** It queries Qdrant for vectors matching "eating pizza" *while filtering* for the specific UUID.
4. **Result:** The exact image is returned.

### üó∫Ô∏è Project Journey & Challenges

**The "Human-in-the-Loop" Pivot**

Initially, we designed the agent to pop up a window asking, *"Who is this?"* whenever it saw a new face. We realized this was unscalable for a gallery of 1,000+ photos.

* **The Pivot:** We shifted to an **Auto-Tagging UUID system**. The agent now assigns anonymous IDs (`uuid-1234`) automatically. We can search by providing a reference photo later, removing the need for manual data entry during ingestion.

**The "Blind Agent" Problem**

Many agent frameworks are text-first and cannot "hold" an image file in memory.

* **The Solution:** We implemented a **File Path Handoff pattern**. The Agent handles the *path string* (e.g., `./images/photo.jpg`), while the custom Python tools handle the actual byte-level processing (OpenCV/DeepFace/Gemini API).

### üõ†Ô∏è Tech Stack

* **Agent Brain:** Google Gemini 2.5 Flash
* **Vision/Biometrics:** DeepFace (Facenet512 + Retina Backend)
* **Memory (Vector DB):** Qdrant
* **Storage:** Cloudinary
* **Backend:** Python 3.10

---

## 4. Setup & Usage

### Prerequisites

Before you begin, ensure you have the following installed and configured:

* **Python 3.10 or higher** - The project requires Python 3.10+
* **Git** - For cloning the repository
* **Google Gemini API Key** - Get yours from [Google AI Studio](https://makersuite.google.com/app/apikey)
* **Cloudinary Account** - Sign up at [Cloudinary](https://cloudinary.com/) for image storage
* **Qdrant Instance** - Use either Docker locally or [Qdrant Cloud](https://cloud.qdrant.io/)

### Installation

Follow these step-by-step instructions to get Gallery Agent running on your local machine:

#### Step 1: Clone the Repository

First, download the project code from GitHub directly into your current directory. Open your terminal or command prompt and run:

```bash
git clone https://github.com/meetbikhani/Nexus_Galllery.git .
```

The `.` at the end tells Git to clone the repository contents directly into your current folder instead of creating a new subfolder. This is useful if you've already created and navigated to a project directory.

#### Step 2: Create a Virtual Environment

A virtual environment keeps your project dependencies isolated from other Python projects on your system. This prevents version conflicts and keeps your global Python installation clean. Create one using:

```bash
python -m venv venv
```

This command tells Python to create a virtual environment module (`-m venv`) in a folder named `venv`. The folder will contain all the project-specific Python packages and interpreters.

#### Step 3: Activate the Virtual Environment

Before installing dependencies, you need to activate the virtual environment. This tells your terminal to use the Python interpreter and packages from the `venv` folder instead of your system-wide Python installation.

**On Windows:**
```bash
venv\Scripts\Activate
```

**On macOS/Linux:**
```bash
source venv/bin/activate
```

You'll know the virtual environment is activated when you see `(venv)` appear at the start of your command line prompt. This indicates that any Python commands you run will now use the isolated environment.

#### Step 4: Install Required Dependencies

Now that your virtual environment is active, install all the Python packages that Gallery Agent needs to function:

```bash
pip install -r requirements.txt
```

This command reads the `requirements.txt` file (which lists all necessary packages and their versions) and installs them into your virtual environment. You'll see packages like DeepFace, Qdrant client, Cloudinary SDK, Google Generative AI, and many others being downloaded and installed. This process may take a few minutes depending on your internet connection speed.

#### Step 5: Configure Environment Variables

Environment variables store sensitive configuration data like API keys. You need to create a `.env` file specifically inside the `agents` folder. Navigate to the `agents` directory:

```bash
cd agents
```

Then create a new file named `.env` with the following content:

```env
GOOGLE_API_KEY=your_gemini_api_key_here
CLOUDINARY_CLOUD_NAME=your_cloud_name
CLOUDINARY_API_KEY=your_cloudinary_api_key
CLOUDINARY_API_SECRET=your_cloudinary_api_secret
GEMINI_API_KEY=your_gemini_api_key_here
QDRANT_URL=your_qdrant_instance_url
QDRANT_API_KEY=your_qdrant_api_key
```

**Important Notes:**
- Both `GOOGLE_API_KEY` and `GEMINI_API_KEY` should contain the same Gemini API key value
- Do not leave any spaces around the `=` sign
- Replace all placeholder values with your actual credentials

**How to get these credentials:**

* **Google Gemini API Key:** 
  1. Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
  2. Sign in with your Google account
  3. Click "Get API Key" or "Create API Key"
  4. Copy the generated key and paste it for both `GOOGLE_API_KEY` and `GEMINI_API_KEY`

* **Cloudinary Credentials:** 
  1. Sign up at [Cloudinary](https://cloudinary.com/) (free tier available)
  2. After logging in, go to your Dashboard
  3. Find your **Cloud Name**, **API Key**, and **API Secret** displayed prominently
  4. Copy each value into the corresponding field in your `.env` file

* **Qdrant Credentials:** 
  - **Option A - Qdrant Cloud (Recommended for beginners):**
    1. Create a free account at [Qdrant Cloud](https://cloud.qdrant.io/)
    2. Create a new cluster (free tier available)
    3. Once created, click on your cluster to view details
    4. Copy the **Cluster URL** (looks like `https://xyz-abc.aws.cloud.qdrant.io:6333`)
    5. Copy the **API Key** from the cluster settings
  
  - **Option B - Local Docker Installation:**
    1. Install Docker on your machine
    2. Run: `docker run -p 6333:6333 qdrant/qdrant`
    3. Use `QDRANT_URL=http://localhost:6333`
    4. You can leave `QDRANT_API_KEY` empty or remove that line for local setup

#### Step 6: Return to Project Root

After creating the `.env` file in the `agents` folder, navigate back to the root directory of the project:

```bash
cd ..
```

This ensures you're in the correct location to run the application. You should be in the main project directory (where `requirements.txt` is located), **not** inside the `agents` folder.

#### Step 7: Run the Application

Now you're ready to launch the Gallery Agent web interface. From the project root directory, run:

```bash
adk web
```

**Important:** This command must be executed from the project root directory (outside the `agents` folder). The Agent Development Kit (ADK) will start a local web server.

After running this command, you'll see output in your terminal indicating the server is running. Look for a message like:

```
üöÄ Server running at http://localhost:8000
```

Open this URL in your web browser to access the Gallery Agent interface where you can:

* **Upload and store images** in your intelligent gallery with automatic face recognition and semantic tagging
* **Search for photos using text descriptions** like "beach sunset" or "birthday party"
* **Find images by face** by uploading a reference photo of a person
* **Perform hybrid searches** combining both face recognition and semantic queries like "John eating cake"

---

## 5. Usage Examples

### Storing Images

When you access the web interface, select the option to store images. Point the system to a folder containing your photos, and it will:

1. Analyze each image for faces and extract facial features using Facenet512
2. Generate detailed semantic summaries of the scene using Gemini AI
3. Assign or match face IDs based on similarity thresholds (< 0.5 = same person, > 0.5 = new person)
4. Upload images to Cloudinary for persistent cloud storage
5. Store all metadata (face vectors, summaries, URLs) in Qdrant for lightning-fast retrieval

### Searching Images

**Text-Only Search:**
```
Query: "sunset at the beach"
```
The system searches through all semantic summaries and returns images depicting beach sunset scenes, even if you never manually tagged them with these keywords.

**Face-Only Search:**
```
Reference Photo: [Upload a photo of a person]
Query: (leave empty)
```
The system extracts the face vector from your reference photo and returns all images in your gallery containing that person, regardless of when or where the photos were taken.

**Hybrid Search (The Most Powerful):**
```
Reference Photo: [Upload a photo of Sandeep]
Query: "eating pizza"
```
The system first identifies all photos containing Sandeep using face recognition, then filters those results to only show images where he's eating pizza based on the semantic analysis. This is the true innovation of Gallery Agent‚Äîcombining identity and context seamlessly.

---

## 6. Troubleshooting

### Common Issues

**Issue: "Module not found" errors**
- Solution: Make sure your virtual environment is activated (you should see `(venv)` in your terminal)
- Run `pip install -r requirements.txt` again

**Issue: "Invalid API Key" errors**
- Solution: Double-check your `.env` file is in the `agents` folder
- Verify all API keys are correct with no extra spaces
- Make sure both `GOOGLE_API_KEY` and `GEMINI_API_KEY` have the same value

**Issue: "Cannot connect to Qdrant"**
- Solution: If using Qdrant Cloud, verify your cluster is running and the URL is correct
- If using local Docker, make sure Docker is running: `docker ps` should show the Qdrant container

**Issue: Face recognition not working**
- Solution: Ensure images have clear, visible faces
- The system works best with front-facing portraits
- Low-quality or blurry images may not be processed correctly

---

## 7. Contributing

We welcome contributions! If you'd like to improve Gallery Agent:

1. Fork the repository on GitHub
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes with clear, descriptive messages (`git commit -m 'Add amazing feature'`)
4. Push to your branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request with a detailed description of your changes

---

## 8. License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## 9. Acknowledgments

* **Google Gemini** for providing powerful multimodal understanding capabilities
* **DeepFace** for robust and accurate face recognition technology
* **Qdrant** for lightning-fast vector similarity search
* **Cloudinary** for reliable and scalable image hosting infrastructure

---

**Built with ‚ù§Ô∏è by the Nexus Gallery Team**
